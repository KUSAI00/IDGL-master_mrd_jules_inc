# Data
data_type: 'text'
dataset_name: 'setfit_20news'
data_dir: '../data/20news/'
pretrained: null
task_type: 'classification'

# Output
out_dir: '../out/20news/distilbert_idgl'

data_seed: 1234
seed: 1234

# Model architecture
model_name: 'TextGraphClf'

hidden_size: 128 # Projected size

# Bert configure
use_distilbert: True

# Regularization
dropout: 0.5
gl_dropout: 0.01

# Graph neural networks
bignn: False
graph_module: 'gcn'
graph_type: 'dynamic'
graph_learn: True
graph_metric_type: 'weighted_cosine'
graph_skip_conn: 0.5
update_adj_ratio: 0.9
graph_include_self: False
graph_learn_regularization: True
smoothness_ratio: 0.2
degree_ratio: 0
sparsity_ratio: 0.1
graph_learn_ratio: 0
input_graph_knn_size: 20
graph_learn_hidden_size: 70
graph_learn_epsilon: 0.4
graph_learn_topk: null
graph_learn_num_pers: 5
graph_hops: 2

# Training
optimizer: 'adam'
learning_rate: 0.001
weight_decay: 0
lr_patience: 2
lr_reduce_factor: 0.5
grad_clipping: null
grad_accumulated_steps: 1
eary_stop_metric: 'acc'
pretrain_epoch: 0
max_iter: 10
eps_adj: 4e-2

# Text data only
batch_size: 16
data_split_ratio: '0.8,0.2'
fix_vocab_embed: True
word_embed_dim: 768
top_word_vocab: 10000
min_word_freq: 10
max_seq_len: 256
word_dropout: 0.5
rnn_dropout: 0.5
no_gnn: False

random_seed: 1234
shuffle: True
max_epochs: 50
patience: 10
verbose: -1
print_every_epochs: 1

# Testing
out_predictions: True
save_params: True
logging: True

# Device
no_cuda: False
cuda_id: 0
