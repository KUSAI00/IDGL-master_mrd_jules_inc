# Data
data_type: 'text'
dataset_name: 'setfit_20news'
data_dir: '../data/20news/'
pretrained: null
task_type: 'classification'

# Output
out_dir: '../out/20news/setfit_20news'

data_seed: 1234 # Fixed
seed: 1234

# Model architecture
model_name: 'TextGraphClf'

# Scalable graph learning
scalable_run: True
ratio_anchors: 0.4 # Based on mrd config

hidden_size: 128

# Bert configure
use_bert: False
use_distilbert: True
freeze_bert: True

# Regularization
dropout: 0.5
gl_dropout: 0.

# Graph neural networks
bignn: False
graph_module: 'gcn'
graph_type: 'dynamic'
graph_learn: True
graph_metric_type: 'weighted_cosine'
graph_skip_conn: 0.5
update_adj_ratio: 0.75
graph_include_self: False
graph_learn_regularization: True
smoothness_ratio: 0.2
degree_ratio: 0
sparsity_ratio: 0
graph_learn_ratio: 0
input_graph_knn_size: 400
graph_learn_hidden_size: 70
graph_learn_epsilon: 0.7
graph_learn_topk: null
graph_learn_num_pers: 4
graph_hops: 2

# GAT only
gat_nhead: 8
gat_alpha: 0.2

# Training
optimizer: 'adam'
learning_rate: 0.001
weight_decay: 0
lr_patience: 2
lr_reduce_factor: 0.5
grad_clipping: null
grad_accumulated_steps: 1
eary_stop_metric: 'acc'
pretrain_epoch: 0
max_iter: 3
eps_adj: 8e-3

# Text data only
batch_size: 16
data_split_ratio: '0.9,0.1,0.0' # Not really used as data_utils handles split, but good for placeholder
fix_vocab_embed: True
word_embed_dim: 768
top_word_vocab: 10000
min_word_freq: 10
max_seq_len: 256
word_dropout: 0.5
rnn_dropout: 0.5
no_gnn: False

random_seed: 1234
shuffle: True
max_epochs: 5
patience: 10
verbose: -1
print_every_epochs: 1

# Testing
out_predictions: True
save_params: True
logging: True

# Device
no_cuda: False
cuda_id: 0
